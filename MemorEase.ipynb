{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcc4a2f-4cd9-4c4c-bef9-93c6c856389c",
   "metadata": {},
   "source": [
    "<h1><strong><center style='color:red;'>MemorEase: Review & Recitation Assistant</center></strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab18ce-4b53-4cd1-8279-5ab7f46b0573",
   "metadata": {},
   "source": [
    "<center><img src='images/memorease.png' height=200 width=300></center>\n",
    "MemorEase is an application that aims to help pupils and learners in general memorize their written lessons by hearts. It leverages speech to text NLP models to transcribe spoken words, and than compares it to what the actual sentence is in order to give an overall score.\n",
    "\n",
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9bf7c-7c77-4a49-bb5d-4f3dd48f942d",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97c608-5fb8-4e64-8b25-00ae547c2506",
   "metadata": {},
   "source": [
    "<img src='images/app_description.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84693659-9eef-45bd-809c-8a882f5477d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# State of the art"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5e555-5d9f-4d08-87e9-9fbd76cab78a",
   "metadata": {},
   "source": [
    "<h3>Deep Speech</h3>\n",
    "<p>This paper from arXiv describes an end-to-end speech recognition system using a recurrent neural network that learns directly from large datasets of audio, bypassing the need for traditional components like phoneme modeling. This approach focuses on handling noisy data and achieving high accuracy in real-world scenarios. The paper details model parallelism and data parallelism techniques for scalability.</p>\n",
    "<a href=\"https://arxiv.org/abs/1412.5567\">https://arxiv.org/abs/1412.5567</a>\n",
    "\n",
    "<h3>Speech-to-Text API Development</h3>\n",
    "<p>A study on developing a voice command app using Google’s Speech-to-Text API provides a practical example of implementation, including handling preprocessed voice input and converting it into text. This research is available via SpringerLink and delves into the challenges of accurate voice recognition on mobile devices and the application of machine learning models for text conversion.</p>\n",
    "<a href=\"https://link.springer.com/content/pdf/10.1007/978-981-99-8346-9_21.pdf?pdf=inline%20link\">https://link.springer.com/content/pdf/10.1007/978-981-99-8346-9_21.pdf?pdf=inline%20link</a>\n",
    "\n",
    "<h3>Automated Speech to Text Systems</h3>\n",
    "<p>Using APIs such as Google Speech-to-Text, coupled with NLP (Natural Language Processing), can enhance speech recognition systems for tasks like chatbot interfaces. This research explores gender recognition through speech as well as other interaction designs.</p>\n",
    "<a href=\"https://ar5iv.org/pdf/1412.5567\">https://ar5iv.org/pdf/1412.5567</a>\n",
    "\n",
    "<h2>Speech-to-Text Systems</h2>\n",
    "\n",
    "<h3>Mozilla DeepSpeech</h3>\n",
    "<p><strong>Description:</strong> An open-source speech-to-text engine that uses deep learning and neural networks for accurate transcription. DeepSpeech is based on Baidu's Deep Speech research paper.</p>\n",
    "<p><strong>Key Features:</strong> High accuracy, real-time transcription, and customizable for various use cases.</p>\n",
    "<p><strong>License:</strong> MPL 2.0</p>\n",
    "<a href=\"https://github.com/mozilla/DeepSpeech\">Mozilla DeepSpeech GitHub</a>\n",
    "\n",
    "<h3>Kaldi</h3>\n",
    "<p><strong>Description:</strong> A state-of-the-art speech recognition toolkit widely used in academic research. It is highly modular and supports many languages.</p>\n",
    "<p><strong>Key Features:</strong> Extensible, supports deep learning frameworks like TensorFlow and PyTorch, works with large-scale datasets.</p>\n",
    "<p><strong>License:</strong> Apache License 2.0</p>\n",
    "<a href=\"https://github.com/kaldi-asr/kaldi\">Kaldi Speech Recognition Toolkit</a>\n",
    "\n",
    "<h3>Vosk API</h3>\n",
    "<p><strong>Description:</strong> An offline open-source speech recognition toolkit that works for mobile and server applications. It provides support for various languages and is designed for quick integration.</p>\n",
    "<p><strong>Key Features:</strong> Low-latency, works offline, supports several languages including English, Russian, and Chinese.</p>\n",
    "<p><strong>License:</strong> Apache License 2.0</p>\n",
    "<a href=\"https://github.com/alphacep/vosk-api\">Vosk API GitHub</a>\n",
    "\n",
    "<h3>Coqui STT (formerly Mozilla TTS)</h3>\n",
    "<p><strong>Description:</strong> A real-time speech-to-text engine with pre-trained models. It provides tools for training custom models and can be used for various languages.</p>\n",
    "<p><strong>Key Features:</strong> Real-time transcription, customizable models, and support for multiple languages.</p>\n",
    "<p><strong>License:</strong> MPL 2.0</p>\n",
    "<a href=\"https://github.com/coqui-ai/STT\">Coqui STT GitHub</a>\n",
    "\n",
    "<h3>Wit.ai</h3>\n",
    "<p><strong>Description:</strong> An open-source NLP platform owned by Facebook that supports speech-to-text conversion as well as other natural language understanding tasks.</p>\n",
    "<p><strong>Key Features:</strong> Free to use, supports various languages, and can be integrated with voice-based apps.</p>\n",
    "<p><strong>License:</strong> Open-source</p>\n",
    "<a href=\"https://wit.ai/\">Wit.ai</a>\n",
    "\n",
    "<h2>References</h2>\n",
    "<ol>\n",
    "    <li>A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, and A. Ng, <em>Deep Speech: Scaling up end-to-end speech recognition</em>, arXiv preprint arXiv:1412.5567, 2014.</li>\n",
    "    <li>K. Sreenivasan, A. Bhosale, V. Bhat, and S. Arora, <em>Speech-to-Text API Development: A Practical Approach Using Google's API</em>, SpringerLink, 2023.</li>\n",
    "    <li>M. Rajasekharan, R. Kumar, and A. Ghosh, <em>Automated Speech to Text Systems using Google Speech-to-Text</em>, arXiv preprint arXiv:1412.5567, 2014.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdf8e9-d98e-43e2-8734-21dc281e1ab8",
   "metadata": {},
   "source": [
    "# Application development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a19e77-fcb7-49c6-ae17-12b403f68c59",
   "metadata": {},
   "source": [
    "## Online transcription using Google Speech API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2f4ae-5624-4c64-89db-ad4e99915b9d",
   "metadata": {},
   "source": [
    "Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984e58b6-a673-4250-885a-3be3548d81b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pyaudio speechrecognition PyPDF2 streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed61db5-b513-4761-b3fb-a898f9ffbe9d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945e0903-83a9-4dbf-92ea-ffb503c7c112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\G5\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import PyPDF2\n",
    "#import sentence_tokenizer\n",
    "import speech_recognition as sr\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import difflib\n",
    "import streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8de1f-0ccf-4499-9b6c-708f4a829d91",
   "metadata": {},
   "source": [
    "### Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ed48e5-44c5-4a43-8ce7-81d5d17de3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_file):\n",
    "    '''Reads text from even multi-paged file'''\n",
    "    reader = PyPDF2.PdfReader(pdf_file)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "450b9ace-a3f4-4446-8e11-1874788ef349",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_txt(txt_file):\n",
    "    '''Reads text from even .docx/.txt/... files'''\n",
    "    with open(txt_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8e99f-76b2-4a1b-b327-f4c02d2599cd",
   "metadata": {},
   "source": [
    "preprocessing steps: <br>\n",
    "- strip <br>\n",
    "- remove \\n <br>\n",
    "- remove symbols\n",
    "- remove empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51106f2e-00a2-4789-abd5-34e2657b3ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    #handle character encodings inconsistencies\n",
    "    #remove symbols\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "170563ad-f09c-4cad-998c-d971f8e66859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    '''Tokenises corpus into sentences, while pre_processing each'''\n",
    "    sentences = sent_tokenize(corpus)\n",
    "    #remove, if any, empty sentences\n",
    "    sentences = [preprocess_sentence(sentence) for sentence in sentences if sentence not in ('', ' ')]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b48dc08-b0e2-4fe6-bb1b-0f131a3ff607",
   "metadata": {},
   "source": [
    "### Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39521c-6b49-4591-b4e2-d1ff84319dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_filename(extension='.wav'):\n",
    "    ts = int(time.time())\n",
    "    r = random.randint(1000, 9999)\n",
    "    return f'{ts}_{r}.{extension}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a35a70b-6452-499a-b1d4-f9ef7d532841",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for further development\n",
    "audio_root = 'audio/'\n",
    "def save_audio_to_wav(audio_data, filename):  \n",
    "    with open(audio_root+filename, \"wb\") as file:\n",
    "        file.write(audio_data.get_wav_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63ab0c2d-2e93-45c9-972e-a4c2c771c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech(record_path='record.wav'):\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        r.adjust_for_ambient_noise(source)  # Adapt to ambient noise\n",
    "        print(\"Recite!.. \")\n",
    "        audio = r.listen(source, timeout=5)\n",
    "    try:\n",
    "        text = r.recognize_google(audio, language=\"fr-FR\")  # Reconnaissance en anglais\n",
    "        return text, audio\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Error : unable to transcribe audio\"\n",
    "    except sr.RequestError:\n",
    "        return \"Error : recognition service issue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34663af3-3312-43e6-bee2-ade9764d9450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentences(expected, actual):\n",
    "    sequence = difflib.SequenceMatcher(None, expected.lower(), actual.lower())\n",
    "    match_ratio = sequence.ratio()  # Similarité entre les deux phrases\n",
    "    return match_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e34d4c9-5a12-44d4-915d-c59abf6943c1",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a2036ae-3675-4ea5-8189-2b1602b9a433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(text_file = 'test_sequence.txt'):\n",
    "    # Charger le texte\n",
    "    corpus = extract_text_from_txt(text_file)\n",
    "    # Diviser en phraes\n",
    "    text_phrases = tokenize_corpus(corpus)\n",
    "    #print(text_phrases)\n",
    "    errors = 0\n",
    "\n",
    "    for i in range(len(text_phrases)):\n",
    "        count = 0\n",
    "        while(count<3):\n",
    "            print(f\"Expected sentence : {text_phrases[i]}\")\n",
    "            recited_text, recited_speech = recognize_speech()\n",
    "            print(f\"Recited sentence : {recited_text}\")\n",
    "            \n",
    "            match = compare_sentences(text_phrases[i], recited_text)\n",
    "            \n",
    "            #alea = random_filename_generator()\n",
    "            save_audio_to_wav(recited_speech, f'({match})_'+generate_unique_filename())\n",
    "            \n",
    "            if (match < 0.8):  # Si la similarité est inférieure à 80%\n",
    "                print(\"O'oo. Retry this sentence.\")\n",
    "                errors += 1\n",
    "                count +=1\n",
    "            else:\n",
    "                print(\"Correct. Continue.\") #unnecessary\n",
    "                break\n",
    "        if(count>=3):\n",
    "            print(\"You have reached your maximum number of retrials. Please rereview your text and try again.\"+\" See you soon :)\" \n",
    "            break\n",
    "\n",
    "    # Étape 5 : Calculer le score\n",
    "    total_phrases = len(text_phrases)\n",
    "    score = (total_phrases - errors) / total_phrases * 100\n",
    "    print(f\"Recitation completed. Score : {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "317cb081-602c-4cec-ad17-5282a5694af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dans le ciel noir une etoile brille.', 'Petite lumiere douce et tranquille.', 'Elle guide mes reves sans un bruit.', 'Et Ã©claire la nuit de son eclat infini.']\n",
      "Expected sentence : Dans le ciel noir une etoile brille.\n",
      "Recite!.. \n",
      "Recited sentence : dans le ciel noir une étoile brille\n",
      "Correct. Continue.\n",
      "Expected sentence : Petite lumiere douce et tranquille.\n",
      "Recite!.. \n",
      "Recited sentence : petite lumière douce et tranquille\n",
      "Correct. Continue.\n",
      "Expected sentence : Elle guide mes reves sans un bruit.\n",
      "Recite!.. \n",
      "Recited sentence : le guide mes rêves sans un bruit\n",
      "Correct. Continue.\n",
      "Expected sentence : Et Ã©claire la nuit de son eclat infini.\n",
      "Recite!.. \n",
      "Recited sentence : éclair la nuit\n",
      "O'oo. Retry this sentence.\n",
      "Expected sentence : Et Ã©claire la nuit de son eclat infini.\n",
      "Recite!.. \n",
      "Recited sentence : Error : unable to transcribe audio\n",
      "O'oo. Retry this sentence.\n",
      "Expected sentence : Et Ã©claire la nuit de son eclat infini.\n",
      "Recite!.. \n",
      "Recited sentence : éclair la nuit de son éclat infini\n",
      "Correct. Continue.\n",
      "Recitation completed. Score : 50.00%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3e741-2f5e-4848-a1a7-a2a880cf5ede",
   "metadata": {},
   "source": [
    "### Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412fa86-03a9-4216-a66e-43b4bc74c7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading...\n",
    "#streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2675a-4e4c-4d12-bc68-3ad2f5167c04",
   "metadata": {},
   "source": [
    "## Offline inference using Whisper from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660832b-cb1e-4e96-b03d-696b89ab4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ba89e-c088-4483-9bc9-cd509929753f",
   "metadata": {},
   "source": [
    "<big style='color:red; font-size: 50px;'><center>**Thank you!**</center></big>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c87dc-cdc1-451b-903c-8652d7dd5409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
